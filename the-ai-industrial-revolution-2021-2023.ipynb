{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-17T19:22:44.857546Z","iopub.execute_input":"2023-06-17T19:22:44.859402Z","iopub.status.idle":"2023-06-17T19:22:44.899172Z","shell.execute_reply.started":"2023-06-17T19:22:44.859300Z","shell.execute_reply":"2023-06-17T19:22:44.898102Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\n/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\n/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The term \"Industrial Revolution\" typically refers to a period in history which occurred in the late 18th and early 19th centuries which experienced significant change in the means of production. This period is called a \"revolution\" because of the rapid and extensive changes it brought about in manufacturing, mining, agriculture, and transportation. The word \"industrial\" is used because this period was characterized by a shift from manual labor and agrarian society to one dominated by industry and machine manufacturing.\n\nJust as the Industrial Revolution dramatically altered the fabric of society, a similar phenomenon appears to be unfolding within the realm of artificial intelligence (AI). Over the past two years, AI has experienced a period of unprecedented growth and advancement,  triggering profound changes in our interaction with machines and their societal impact. Tim Sweeny, CEO of Epic Games and a significant figure in the tech industry, encapsulated this transition in a recent tweet. He stated, \"Artificial intelligence is doubling at a rate much faster than Moore’s Law’s 2 years, or evolutionary biology’s 2M years. Why? Because we’re bootstrapping it on the back of both laws. And if it can feed back into its own acceleration, that’s a stacked exponential\" (Sweeney, 2023). \n\nIn the tweet he quoted, OpenAI announced the release of an implementation of Consistency Models, a new type of generative model that achieves high sample quality without the need for adversarial training (Song, Dhariwal, Chen, & Sutskever, 2023). This innovation is a significant breakthrough because adversarial training, although powerful, can be computationally demanding and difficult to optimize, thereby limiting its practicality in various applications. The capacity to produce high-quality samples without adversarial training means more efficient learning models and widens the potential for AI usage across diverse fields. This represents another large step forward in AI capabilities and aligns with Sweeny's point about the rapid rate of AI advancement. As AI development feeds back into its own acceleration, we may indeed be witnessing what Sweeny describes as a \"stacked exponential\" growth.\n\nThe expected growth of computing power has traditionally been benchmarked against Moore's law. However, in the last two years we have seen a surge in GPU power that dramatically outpaces this prediction (Moore, 2022). What are the factors driving this rapid pace of innovation? And what does this acceleration mean for the future of AI development? By exploring this modern 'AI Industrial Revolution,' this essay will delve into the factors fueling AI's rapid evolution and consider the potential implications of this exponential growth.","metadata":{}},{"cell_type":"markdown","source":"# Theoretical Foundations","metadata":{}},{"cell_type":"markdown","source":"\nMoore's Law, named after Intel co-founder Gordon E. Moore, predicts that the number of transistors on integrated circuits doubles approximately every two years. Historically, this has been a rough indicator of computational power growth, but it doesn't directly correlate with complex tasks like machine learning model training or inference, where factors like algorithms, data I/O speeds, memory design, and power efficiency also play major roles.\n\nRecently, the 'end of Moore's Law' has been discussed as transistor miniaturization faces physical and economic hurdles. Gordon Moore himself pointed out in 2005 that the atomic nature of materials presents a fundamental limitation (Tardi, 2023). As transistors get smaller, the energy required for cooling exceeds the energy passing through them. With these challenges, advancements have slowed, signaling that Moore's Law's era might be nearing its end.\n\nIn 2019 and 2020 a lot of attention was given to the end of Moore's law. An article titled 'We’re not prepared for the end of Moore’s Law' [7] was published in MIT Technology review essentially declaring Moore's law dead and as a result \"signals the decline of computers as a general purpose technology\". Another article containing the chart on the left arguing that the doubling time is now 3.5 years and presenting a more accurate description of the growth curve. But that isn't to say the predictions have been inaccurate. The chart on the right shows Moore's law predictions vs actual doubling time. The emphasis has been around whether this is sustainable going into the 2020's and whether continued investments will pay off. ","metadata":{}},{"cell_type":"markdown","source":"As we entered the 2020s, much of the conversation within the computer hardware community shifted towards questioning the sustainability of Moore's Law into the next decade. In 2019 and 2020, several articles, including an influential piece titled 'We’re not prepared for the end of Moore’s Law' (Hoffman, 2020), published in the MIT Technology Review, raised alarm bells about the supposed death of the once-reliable prediction.\n\nThe chart on the left, taken from the sixth edition of \"Computer Architecture: A Quantitative Approach\" by Hennessy and Patterson (2018), suggests a slowing pace in the doubling of transistors. This refined depiction of the growth trajectory integrates other factors and laws into its prediction, thus offering a more nuanced understanding of technology progression. The chart aligns with recent investigations indicating that the rate of transistor doubling has extended to approximately every 3.5 years, a significant departure from Moore's Law original two-year prediction (Barry, 2023). This finding underscores the evolving nature of technological advancement, presenting a challenge to the future relevance of Moore's Law in its traditional form.","metadata":{}},{"cell_type":"markdown","source":"<table>\n  <tr>\n    <td align=\"center\">\n      <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0780a30a-e1dc-4622-90aa-b5ec3f7c07c6_925x600.png\" width=\"450\">\n    </td>\n    <td align=\"center\">\n      <img src=\"https://i.redd.it/gtvlzsimcf981.png\" width=\"400\">\n    </td>\n  </tr>\n</table>\n","metadata":{}},{"cell_type":"markdown","source":"The landscape of computing has evolved significantly since the inception of Moore's Law, introducing new paradigms that influence computational performance beyond mere transistor counts. For instance, the concept of Complex Instruction Set Computing (CISC) and Reduced Instruction Set Computing (RISC) plays a vital role in processor architecture design. CISC, characterized by a large set of instructions, was prominent during the early days of computing but has since given way to RISC architectures, which prioritize a smaller set of instructions executed more quickly, leading to better power efficiency and performance (Panigrahi, 2023). Most modern CPUs, including those by Intel and AMD, use a hybrid approach, while GPUs and many AI accelerators lean towards RISC.\n\nFurther, other physical laws and principles also affect computing performance. Dennard scaling, which predicts that power density (energy per unit area) would remain constant as transistors shrink, also reached its limit in the 2005-2006 time period (Platt, 2018). As transistors have become smaller, power leakage has increased, leading to higher power density, increased heat, and reduced performance per watt. This breakdown, often called the \"end of Dennard scaling,\" then shifted the focus towards multi-core and parallel processing designs to continue the trajectory performance improvements.\n\nHowever, multi-core processing designs hit their own roadblock, referred to as Amdahl's law. It states that the maximum improvement in performance due to parallelization is limited by the portion of the program that cannot be parallelized. In other words, even with an infinite number of processors, there's a limit to how much speedup you can achieve if any part of your computation must be performed sequentially (Brans, n.d.).\n\nDespite the historical accuracy of Moore's law, the recognition of slowing caused many to look to future speedups coming from acceleration and not increased transistor count. ","metadata":{}},{"cell_type":"markdown","source":"# Historical Context","metadata":{}},{"cell_type":"markdown","source":"The term \"artificial intelligence\" was first introduced by scientists John McCarthy, Claude Shannon, and Marvin Minsky at the Dartmouth Conference in 1956. With optimistic expectations for the field's potential, Marvin Minsky boldly proclaimed in a 1970 Life magazine article that within the next three to eight years, machines with average human intelligence would exist. The hype surrounding this forecast ignited an investment wave in the 1970s, which culminated in an AI bubble. However, when this bubble burst in the early 1980s, AI development regressed to the confines of research labs, and the field entered a long-lasting \"AI Winter\" (Jotrin Electronics, 2022).\n\nThis winter was primarily a result of inadequate computational power and data availability necessary for complex AI model training. During this period, Central Processing Units (CPUs) performed the bulk of computations. CPUs, although efficient at handling a wide array of tasks, were not suited for large-scale AI operations. Furthermore, the algorithms and techniques utilized during this time were still in their infancy, lacking the sophistication and effectiveness of those developed in later years.\n\nMoore's Law had been the guiding principle for the advancement of these CPUs. However, the demands of AI computations quickly exceeded the capabilities of these CPU architectures which caused much slower progress than originally anticipated by Minsky. \n\nThe birth of the Graphics Processing Unit (GPU) by Nvidia in 1999 marked a significant shift in this trajectory. Initially used to accelerate 3D graphics for PC video games, GPUs offloaded computational work from the CPU, enhancing processing speeds. However, their application in AI model training wasn't recognized until a decade later.\n\nIn 2009, Geoffrey Hinton recommended using GPUs for model training, and Stanford University researchers Rajat Raina, Anand Madhavan, and Andrew Ng, published a paper illustrating the superior computational power of modern GPUs compared to multi-core CPUs for deep learning. Hinton labeled them as \"the future of machine learning\", but the tipping point came in 2012 when AlexNet, a neural network model developed by Hinton and his student Alex Krizhevsky, won the ImageNet competition with record image recognition accuracy using Nvidia's GPU hardware (Jotrin Electronics, 2022).\n\nThis marked a paradigm shift, elevating GPUs to the gold standard for AI advancements. It also highlighted how the AI hardware landscape was beginning to accelerate far beyond the incremental advancements predicted by Moore's Law, thereby setting the stage for the next decade of unprecedented growth in AI capabilities.\n\nHardware has since been the silent workhorse in the world of AI, defining the speed and efficiency of both model training and deployment. The choice of hardware can dramatically affect an AI model's learning curve, determining how rapidly it can digest and learn from data. Equally, the hardware used in deployment affects the speed at which these AI models can produce predictions and respond to inputs, a crucial factor in many real-time applications.\n\nThe November 2022 release of ChatGPT by OpenAI has highlighted the essential role of hardware in driving AI performance. The high-level functionality of ChatGPT demands significant memory and storage capacity. For instance, this system was trained on an extensive network of 10,000 NVIDIA A100 HPC (high-performance computing) accelerators, each of which is a $12,500 tensor core Graphics Processing Unit (GPU) (Kandel, 2023). A case in point is the third version of ChatGPT, which features an astounding 175 billion parameters and calls for a data capacity of 45 terabytes during its training stage. This exceeds the memory capabilities of even the most powerful GPUs typically used in system training, necessitating the concurrent operation of multiple processors. While the hardware used in deployment can vary significantly based on specific application requirements, the selection of hardware is indisputably a key consideration in the realm of AI.\n","metadata":{}},{"cell_type":"markdown","source":"# The Boom of AI Hardware (2021-2023)","metadata":{}},{"cell_type":"markdown","source":"## 2021","metadata":{}},{"cell_type":"markdown","source":"As we entered the new decade, the stage was set for AI hardware to take a giant leap forward. Not only was the true power about to be put to the test with the first version of Machine Learning Performance benchmark tests (MLPerf v1.0) on the horizon, there was significant capital allocated to AI hardware development. Although there was technically a slight decrease in the number of equity funding deals vs 2020 (2,384 deals in 2021 versus 2,450 in 2020), the amount of capital invested in AI hardware companies globally almost doubled from 36 billion in 2020 to 68 billion in 2021. Market research reports later found that in 2021 the highest demand in AI hardware was be for processors (65%) rather than storage or network devices (Precedence Research, 2022).\n\n![](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Share-By-Type-2021.jpg)\n\nIn 2021, several major advancements were made that further fueled the AI boom. To start, in February, Google released TensorFlow 3D, designed to help businesses develop and train models capable of comprehending 3D scenes. This offering signified an expansion in the AI model development ecosystem, with TensorFlow employing the raw power of GPUs for model training.\n\nMarch saw a landmark collaboration between Nvidia and Harvard, with the development of an AI toolkit called AtacWorks. This toolkit was a testament to Nvidia's determination to tailor AI hardware to handle complex tasks such as genome analysis, thus significantly reducing associated costs and time.\n\nThen in April, Cerebras unveiled an AI supercomputing processor containing an unprecedented 2.6 trillion transistors. This powerful computational device underscores the intensifying demand for advanced AI hardware to keep up with the increasingly intricate tasks.\n\nIn May, Google announced the introduction of their fourth-generation tensor processing units (TPUs) for AI and machine learning workloads. TPUs, designed specifically to optimize AI computation, stood as Google's response to the rising dominance of GPUs.\n\nJune brought another pivotal development when Mythic launched an AI processor that required ten times less power than a conventional system-on-chip or GPU. This introduction marked a shift towards creating more energy-efficient hardware solutions for AI, an important consideration as energy costs and environmental impacts become more of a concern.\n\nIn October, Apple also continued upgrades to the M1 series chips released only a year earlier and already touted as the most powerful chips Apple had ever built. Most notably, both the M1 Pro and M1 Max chips came equipped with the standard 16-core Neural Engine but further ehnanced for accelerating on-device machine learning, indicative of Apple's investment in advancing machine learning technology through their existing products (\"Introducing M1 Pro and M1 Max,\" 2021).\n\nNovember showcased advancements from both Nvidia and Amazon. Nvidia announced Omniverse Avatar, a platform harnessing AI hardware capabilities to create real-time interactive avatars, signifying an innovative use of AI hardware. Simultaneously, Amazon unveiled its Graviton3 processors for AI inferencing, illustrating an industry trend towards using AI-specific processors for distinct tasks such as inference (Sharma, 2021).","metadata":{}},{"cell_type":"markdown","source":"### Hardware Summary 2021\n\n| Hardware | Company | Key Features |\n|---|---|---|\n| Perlmutter supercomputer | NERSC | Advanced supercomputer for scientific research, with more than 7000 Nvidia A100 GPUs |\n| Grace CPU | Nvidia | Arm-based CPU designed for AI and high-performance computing |\n| DGX SuperPOD | Nvidia | AI supercomputer for enterprise-level AI training and inference |\n| Google TPU v4 | Google | Tensor Processing Unit designed for Google's data centers, offering high performance in AI tasks |\n| Habana Gaudi AI Training Processor | Intel | High-performance AI processor focused on training tasks |\n| Wafer Scale Engine 2 (WSE-2) | Cerebras Systems | Extremely large chip (size of a dinner plate) designed for AI tasks, contains 2.6 trillion transistors |\n| Snapdragon 888 5G | Qualcomm | Mobile platform with integrated 5G and an improved AI Engine |\n| M1 Pro/Max Chip | Apple | 16-core Neural engine optimized for ML acceleration |\n","metadata":{}},{"cell_type":"markdown","source":"## 2022","metadata":{}},{"cell_type":"markdown","source":"\nThroughout 2022, the AI hardware landscape saw an array of impressive launches from leading tech companies and startups alike. Nvidia announced the release of their new DGX Station, DGX-1, and DGX-2 built on state-of-the-art Volta GPU architecture (Gupta, 2022). Nvidia also announced the release of the H100 data center GPU, the flagship product for the new Hopper architecture. All of these components are specifically designed for deep learning training, accelerated analytics, and inference (Fu, 2022).\n\nIntel’s Habana Labs released the second generation of their deep learning processors for training and inference — Habana Gaudi2 (Gupta, 2022). IBM launched their first Telum Processor-based system, IBM z16, aimed at improving performance and efficiency for large datasets and featuring on-chip acceleration for AI inference (Fu, 2022).\n\nIn March and June, Apple also made significant strides in their hardware capabilities, unveiling the M1 Ultra and M2 chip, both next-generation enhancements of their breakthrough M1 chip. The M1 Ultra doubled the number of previous of neural engine cores from 16 to 32 (\"Apple unveils M1 Ultra,\" 2022). The new mac standard neural engine in M2 can process up to 15.8 trillion operations per second, 40% faster than the prior year. (\"Apple unveils M2,\" 2022).\n\nOn AI Day, Tesla revealed its powerful Dojo chip, designed for faster training and inference in self-driving cars (Gupta, 2022). AMD, though not traditionally focused on AI, released Zen 4, a new version of their Zen microarchitecture built on a 5 nm architecture, and introduced a new line of PC processors for machine learning capabilities (Fu, 2022). Meanwhile, Cerebras Systems launched their AI supercomputer, Andromeda, aiming to accelerate academic and commercial research (Gupta, 2022).\n\nIn the same vein, SambaNova Systems announced the shipping of the second generation of the DataScale system—SN30. The system, powered by the Cardinal SN30 chip, is built for large models with more than 100 billion parameters and capable of handling both 2D and 3D images (Fu, 2022).\n\nBy mid-2022 we had a pretty good understanding of the state of the market for the prior year and where things were headed. The AI hardware market was valued at 10 billion in 2021 and was projected to grow to almost 90 billion by 2030 (Precedence Research, 2022).\n\n![](https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg)\n\nThe AI Hardware Summit held in September 2022 showcased the emergent trend of Edge AI, pointing out its potential as a major avenue for growth and performance improvement. Edge AI, which refers to deploying AI applications on devices throughout the physical world, has seen remarkable advancement due to the maturation of deep learning and enhanced computing power. The Summit also highlighted how AI chips have now advanced to the level of detecting human emotions, emphasizing the impressive strides being made in edge computing and object detection. Furthermore, a noticeable shift was identified toward Tensor Processing Units (TPUs) in Edge AI, with more vendors beginning to adopt TPUs as AI accelerators.\n\nOne of the key themes of the Summit was the rise of foundation models in AI, signaling a new era in AI development. These models, trained on massive amounts of data and adapted for multiple applications, have started to replace the task-specific models that previously dominated the AI landscape. Although still relatively nascent and not entirely understood, foundation models have shown tremendous potential and are being deployed at scale.\n\nAnother pivotal discussion point was the evolving large-scale AI infrastructure. The focus was on developing high-performance computers with AI-optimized accelerators, efficient software for AI development, robust data center environments, and even innovative cooling solutions for high-density computing equipment (Fu, 2022).\n\nIn 2022, one of the noteworthy advancements in AI wasn't a physical piece of hardware, but a sophisticated language model known as ChatGPT, trained by OpenAI. Despite its primary role as a web application, its existence and performance have significant implications for the hardware domain. ChatGPT, which requires substantial computational power for both training and inference as previously mentioned, is a testament to the increasing demand for advanced hardware capable of supporting such large models. Training these large models often requires specialized hardware and would not be possible without prior advancements in GPUs or TPUs that can handle a large amount of data and perform parallel computations. Moreover, the inference stage often requires powerful servers for hosting the models, as well as efficient hardware capable of quickly processing requests in real-time. The success of ChatGPT underscores the intertwined relationship between AI software and hardware advancements, where each drives progress in the other.","metadata":{}},{"cell_type":"markdown","source":"### Hardware Summary 2022\n\n| Hardware | Company | Key Features |\n|---|---|---|\n| DGX Station, DGX-1, and DGX-2 | Nvidia | AI supercomputers built on Volta GPU architecture for deep learning, analytics and inference |\n| H100 data center GPU | Nvidia | Flagship product built on the new Hopper architecture, ideal for large-scale machine learning and deep learning workloads. |\n| Habana Gaudi2 | Intel | Deep learning processor for training and inference, built with 7nm technology |\n| IBM z16 | IBM | First Telum Processor-based system, for improving performance and efficiency for large datasets, features on-chip acceleration for AI inference |\n| Zen 4 | AMD | Microarchitecture built on a 5 nm architecture, introduced for machine learning capabilities |\n| Dojo Supercomputer | Tesla | Revealed for faster training and inference in self-driving cars, claims to outperform multiple GPUs |\n| Andromeda Supercomputer | Cerebras Systems | Combines 16 Cerebras CS-2 systems for academic and commercial research, performs one quintillion operations per second |\n| SN30 Datascale | SambaNova Systems | Second generation of the DataScale system, powered by Cardinal SN30 chip, built for large models with more than 100 billion parameters |\n| M1 Ultra, M2 | Apple | 32-core pro model neural engine, 40% faster standard Neural Engine over previous year, 15.8 trillion operations per second |","metadata":{}},{"cell_type":"markdown","source":"## 2023","metadata":{}},{"cell_type":"markdown","source":"The boom in AI hardware in 2023 is characterized by a proliferation of new platforms engineered for high performance, extreme scalability, energy efficiency, and sophisticated deep learning techniques. These advancements have unlocked new frontiers in the AI and machine learning landscape, with significant contributions coming from industry powerhouses such as Google, NVIDIA, Intel, AMD, Apple, and Meta.\n\nAs a result of OpenAI's release of ChatGPT in late 2022, the following six months have seen an explosion in AI advancements. While the 2023 AI Hardware summit has not been held at the time of writing, we can look to the most recent developer conferences from Google, Apple, and Microsoft to give us an idea of the types of advancements we are going to see in the latter end of 2023. Google I/O, Microsoft Build, and Apple WWDC are the yearly flagship conferences where developers in particular get a deeper dive into the software and hardware that will launch later in the year. Google and Microsoft are fully embracing and participating in the AI race with new virtual assistants, product features, and open large language models just to name a few. Apple, a bit more subtle in the AI race, did not once mention the term \"artificial intelligence\" at this years WWDC (Greenburg, 2023). Instead they unveiled numerous software improvements for machine learning across the device ecosystem along with the upgraded M2 Ultra's 32-core neural engine touted as 40% than the prior year 32-core model. (\"Apple introduces M2 Ultra,\" 2023).\n\nGoogle made a giant leap in its Cloud TPU v4, offering a staggering 10x increase in machine learning system performance compared to its predecessor, TPU v3. With innovative interconnect technologies and domain-specific accelerators, the TPU v4 not only amplifies performance, but it also champions energy efficiency, leading to a reduction in CO2 emissions. Notably, the TPU v4 is tailored for large language models such as LaMDA, MUM, and PaLM, with the PaLM model delivering 57.8% of peak hardware floating-point performance over 50 days of training on the TPU v4 (Jouppi & Patterson, 2022).\n\nNvidia marked a substantial milestone with its Grace CPU Superchips, finding a place in the UK-based Isambard 3 supercomputer. This setup, featuring 384 Arm-based NVIDIA Grace CPU Superchips, commands a total core count exceeding 55,000. It delivers FP64 performance within a remarkable power envelope of under 270kW. The incorporation of Arm Neoverse V2 cores offers a high-performance edge, as the Grace chips are projected to have superior speed and memory bandwidth compared to their counterparts (Kennedy, 2023).\n\nIntel, with its Meteor Lake chips, embedded Vision Processing Units (VPUs) across all variants, thereby offloading AI processing tasks from the CPU and GPU to the VPU. This move resulted in increased power efficiency and ability to handle complex AI models, providing benefits for power-hungry applications such as Adobe suite, Microsoft Teams, and Unreal Engine [].\n\nAMD introduced an AI chip called MI300X, described as \"the world's most advanced accelerator for generative AI\". This introduction is expected to compete head-on with Nvidia's AI chips and generate interest from major cloud providers. Simultaneously, AMD initiated high-volume shipping of a general-purpose central processor chip named \"Bergamo\", adopted by Meta Platforms and others for their computing infrastructure [].\n\nMeta made its foray into AI hardware by unveiling its first custom-designed chips, the Meta Training and Inference Accelerator (MTIA) and the Meta Scalable Video Processor (MSVP). These chips, optimized for deep learning and video processing, underpin Meta's plans for a next-gen data center optimized for AI, illustrating its dedication to crafting a fully integrated AI ecosystem [].\n\nSignificant developments in 2023 also included the rise of interactive Large Language Models like Google's BARD and Bing AI. These large-scale models, designed for interactive and responsive tasks, leverage the power and efficiency of the latest AI hardware, thus widening their practical applications.\n\nThe hardware is powring these large language models and making them possible.\n\nWhile these advancements in 2023 are indeed significant, it's important to note that the AI Hardware Summit for the year is yet to occur, indicating that we don't have the full picture of all the developments in the field for this year. As such, the current state of AI hardware should be viewed as a work in progress, awaiting further updates and advancements.","metadata":{}},{"cell_type":"markdown","source":"| Hardware | Company | Key Features |\n| -------- | ------- | ------------ |\n| Meteor Lake chips with Vision Processing Units (VPUs) | Intel | Embedded VPUs in all chips for increased power efficiency and the ability to handle complex AI models |\n| MI300X AI Chip and Bergamo Processor | AMD | Introduced the MI300X, the world's most advanced accelerator for generative AI, and started high-volume shipping of the Bergamo central processor chip |\n| Meta Training and Inference Accelerator (MTIA) and Meta Scalable Video Processor (MSVP) | Meta | Unveiled custom-designed AI chips optimized for deep learning and video processing and discussed plans for a next-gen data center optimized for AI |\n| M2 Ultra | Apple | 32-core neural engine, 31.6 trillion operations per second |\n| Google Cloud TPU v4** | Google | Exascale ML performance, 4096 chips, dynamic OCS reconfigurability, hardware support for embeddings, 3D torus interconnect |\n| NVIDIA Grace CPU Superchips in Isambard 3 | Nvidia | 384 Arm-based NVIDIA Grace CPU Superchips, >55,000 cores, FP64 performance, <270 kW power consumption, Arm Neoverse V2 cores |\n| Interactive Large Language Models | Google (BARD), Microsoft (Bing AI) | Large-scale models designed for interactive and responsive tasks, leveraging the power and efficiency of the latest AI hardware |\n","metadata":{}},{"cell_type":"markdown","source":"# Important AI Benchmarks","metadata":{}},{"cell_type":"markdown","source":"Moore's Law has been a useful guideline for hardware development, but predicting the pace of improvement in machine learning performance is much more complex due to these many additional factors. This is one reason why more relevant benchmarks like MLPerf are so valuable - they offer a more holistic view of system performance for more relevant tasks.\n\nThere are several benchmarks that are commonly used today to evaluate the performance of ML/AI hardware specifically. MLPerf is one of the most popular of the last two years. Developed by a consortium of tech companies in 2018, MLPerf benchmarks measure the speed of machine learning software and hardware.\n\nAnother benchmark that has gained attention recently is the AI Benchmark, which is designed specifically for AI tasks on mobile devices. This benchmark measures the speed, accuracy, and power efficiency of AI algorithms on various hardware platforms, including CPUs, GPUs, and dedicated AI accelerators.\n\nThe Compute Architecture Benchmark Review (CARB) is another initiative that aims to provide clear, consistent performance benchmarks for various computational tasks, including machine learning.\n\nHowever, these benchmarks focus mainly on the operational aspect of machine learning - i.e., how quickly a given piece of hardware can perform a specific task. They do not necessarily reflect the research or development aspect of machine learning - i.e., how quickly a new model can be developed, trained, and optimized.\n\nMetrics like time-to-solution or time-to-accuracy are more indicative of the research productivity, which often involves multiple iterations of model development, training, and optimization.\n\nBenchmarks such as MLPerf HPC provide insights into the performance of hardware on High Performance Computing (HPC) workloads, which include tasks like weather forecasting, quantum mechanics, and molecular dynamics, along with machine learning tasks.","metadata":{}},{"cell_type":"markdown","source":"# Comparing the Pace: Moore's Law vs. AI Hardware Growth","metadata":{}},{"cell_type":"markdown","source":"Although MLPerf launched in 2018, it wasn't until the very end of 2020 that it was properly scaled and standardized into the ML Commons consortium. This is why the MLPerf tests of 2021 are referred to as MLPerf v1.0. MLPerf consists of eight benchmark tests: image recognition, medical-imaging segmentation, two versions of object detection, speech recognition, natural-language processing, recommendation, and a form of gameplay called reinforcement learning. MLPerf is often referred to as \"the Olympics of machine learning\" because computers and software from 21 different companies compete on any or all the tests [IEEE]. This incentivizes hardware companies like Nvidia to put their best foot forward.\n\nIn 2022 an IEEE Spectrum article came out following MLPerf v2.0, the June 2022 benchmark test results, that specifically described the rapid outpacing of AI hardware and training times compared to Moore's law. \n\n![](https://spectrum.ieee.org/media-library/a-chart-shows-six-lines-of-various-colors-sweeping-up-and-to-the-right.jpg?id=30049159&width=1580&quality=80)\n","metadata":{}},{"cell_type":"markdown","source":"Based on the release of the 2023 MLPerf results, the pace of AI innovation is not only continuing but accelerating at a rate much faster than previously predicted. NVIDIA's AI platform has shown a considerable performance increase over its 2022 results, reaffirming Tim Sweeny's statement that AI is \"doubling at a rate much faster than Moore’s Law’s 2 years.\"\n\nIn 2022, NVIDIA's AI platform, powered by the A100 Tensor Core GPU, demonstrated significant versatility and efficiency across all eight MLPerf benchmarks. It achieved the fastest time to train on four out of eight tests and was found to be the fastest on a per-chip basis on six out of the eight tests. This performance was attributed to full-stack innovations spanning GPUs, software, and at-scale improvements, delivering 23x more performance in 3.5 years since the first MLPerf submission.\n\n![](https://blogs.nvidia.com/wp-content/uploads/2022/04/MLPerf-inference-April-22-FINAL2-1-1536x663.jpg.webp)\n\nFast forward to 2023, the results are even more impressive. The newly introduced NVIDIA H100 Tensor Core GPUs, running on DGX H100 systems, not only achieved the highest performance in every test of AI inference but also saw a performance gain of up to 54% since their debut in September [25]. This unprecedented progress was in part due to NVIDIA's Transformer Engine, a testament to the company's commitment to optimizing software and hardware innovations to push the boundaries of AI performance.\n\n![](https://blogs.nvidia.com/wp-content/uploads/2023/04/H100-GPU-inference-performance-MLPerf-1536x857.jpg)\n\nSpecifically, in the healthcare domain, the H100 GPUs have improved performance by 31% since September on the 3D-UNet benchmark, used for medical imaging. Additionally, the H100 GPUs powered by the Transformer Engine excelled in the BERT benchmark, a transformer-based large language model, significantly contributing to the rise of generative AI [25].\n\nFurthermore, the NVIDIA L4 Tensor Core GPUs, which debuted in the MLPerf tests, ran over 3x the speed of prior-generation T4 GPUs, demonstrating another significant leap in AI performance.\n\nThis rapid advancement showcases how technology evolves by \"bootstrapping\" on previous laws, as Sweeny noted. As these technologies accelerate their development, they also enable new levels of efficiency and capabilities that would have been inconceivable in previous years. In essence, these advancements show that we are not only building upon the foundations laid by Moore's Law but are also accelerating beyond it, providing the fertile ground necessary for the exponential growth of AI technologies.","metadata":{}},{"cell_type":"markdown","source":"# Implications and Consequences of this Accelerated Pace","metadata":{}},{"cell_type":"markdown","source":"While this accelerated growth in AI brings numerous benefits in fields such as healthcare, finance, climate modeling, and more, it also surfaces pressing questions around the control, understanding, and potential implications of these rapidly evolving technologies. As we stand at the forefront of this technological surge, it's crucial to probe these concerns and address them preemptively.\n\nThe growth of AI, as Sweeney alludes, follows a pattern of 'stacked exponential acceleration'. This not only implies an extraordinary rate of progress, dwarfing the comparatively linear advancements predicted by Moore's Law or the glacial pace of biological evolution, but it also suggests that AI could potentially reach, or even surpass, human-level intelligence. This is not a proposition to be taken lightly. In the hands of well-intentioned researchers and developers, AI systems with capabilities beyond human intelligence could yield unprecedented advancements and solve some of the world's most challenging problems. However, if mishandled, such powerful systems could pose profound ethical and existential risks.\n\nTherefore, as we continue to harness the fruits of Moore's Law and biological evolution in our pursuit of AI advancement, it is imperative that we maintain a vigilant and proactive stance towards its development. This involves putting in place robust mechanisms to regulate and monitor the evolution of these systems, fostering transparency and open dialogue about their potential implications, and ensuring that the benefits of AI are equitably distributed. In this way, we can prevent the unwieldy growth of AI from eclipsing our collective wisdom and instead guide it towards the greater good of humanity.\n\nOn the other hand, the prospect of AI reaching or exceeding human intelligence, while sobering, also opens up a plethora of exciting possibilities. It encourages us to reimagine the boundaries of what can be achieved, to envision a future where our most intractable challenges have been surmounted, and to redefine what it means to be human in a world where our creations might mirror us in intelligence, or even surpass us.\n\nThe remarkable growth and development of AI thus underscores the need for us to carefully steward this technology. It's a call for us to apply our collective intelligence and wisdom towards ensuring that AI develops in a way that is beneficial for all of humanity, mitigating the risks and maximizing the benefits. It is, ultimately, a call for responsible and informed evolution of the technology that is redefining our world.","metadata":{}},{"cell_type":"markdown","source":"# Future Predictions and Conclusion","metadata":{}},{"cell_type":"markdown","source":"Comparing modern hardware benchmarks to Moore's Law, it's clear that while Moore's Law provides a valuable historical context for the evolution of computing power, it doesn't fully encapsulate the multifaceted nature of the explosion in hardware acceleration. Current benchmarks like MLPerf provide a much more nuanced view of this phenomenon, taking into account the complexity of the tasks being performed, the efficiency of the algorithms used, and the intricacies of the hardware and software designs.\n\nThe unpredictable pace of AI hardware acceleration emphasizes not just the immense potential of the field, but also the critical importance of our ongoing commitment to responsible stewardship. The rise of potent systems like ChatGPT reminds us that what was very recently in the realm of science fiction has indeed become our reality. The blend of awe and concern we experience today highlights our responsibility to manage these technological leaps wisely. The swift rate of AI development, outpacing both Moore's Law and biological evolution, invites urgent dialogues about control, understanding, and potential implications of AI systems that can reach or surpass human-level intelligence. Rather than being passive observers of this rapid acceleration, we must actively shape our relationship with AI, fostering a future where the technology serves humanity's best interests, mitigates risks, and contributes to a more informed, equitable, and innovative world. \n\nGoing forward, the question is not just how quickly we can double the number of transistors on a chip, but how we can best optimize the entire system - hardware, software, and algorithms - to deliver the most effective performance for the full range of tasks espeically those that revolve around model training. This reflects an overall shift from a focus on hardware alone (as epitomized by Moore's Law) to a more holistic view of computing performance. The future of computing is bright and the current incentives around AI hardware in particular will continue to drive performance innovations at speeds we have not yet experienced in the age of computing.","metadata":{}},{"cell_type":"markdown","source":"# Sources","metadata":{}},{"cell_type":"markdown","source":"[1] Dilmengani, C. (2023, June 17). AI chip makers: Top 10 companies in 2023. Retrieved from https://research.aimultiple.com/ai-chip-makers/\n\n[2] Edwards, B. (2023, May 24). The lightning onset of AI—what suddenly changed? An Ars Frontiers 2023 recap. Ars Technica. Retrieved from https://arstechnica.com/information-technology/2023/05/the-lightning-onset-of-ai-what-suddenly-changed-an-ars-frontiers-2023-recap/\n\n[3] Fu, J. (2022, September 29). AI frontiers in 2022. Better Programming. Retrieved from https://betterprogramming.pub/ai-frontiers-in-2022-5bd072fd13c\n\n[4] Greenberg, M. (2023, June 6). The best AI features Apple announced at WWDC 2023. VentureBeat. Retrieved from https://venturebeat.com/ai/the-best-ai-features-apple-announced-at-wwdc-2023/\n\n[5] Gupta, A. (2022, March 8). 7 Best AI Hardware Released in 2022. Analytics India Magazine. Retrieved from https://analyticsindiamag.com/7-best-ai-hardware-released-in-2022/\n\n[6] Hamblen, M. (2023, February 16). ChatGPT runs 10K Nvidia training GPUs with potential for thousands more. Fierce Electronics. Retrieved from https://www.fierceelectronics.com/sensors/chatgpt-runs-10k-nvidia-training-gpus-potential-thousands-more\n\n[7] Hoffman, K. (2020, February 24). We're not prepared for the end of Moore's law. MIT Technology Review. Retrieved from https://www.technologyreview.com/2020/02/24/905789/were-not-prepared-for-the-end-of-moores-law/\n\n[8] Jouppi, N., & Patterson, D. (2022, June 29). TPU v4 enables performance, energy, and CO2e efficiency gains. Google Cloud Blog. Retrieved from https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains\n\n[9] Kandel, A. (2023, April 7). Secrets of ChatGPT's AI Training: A Look at the High-Tech Hardware Behind It. Retrieved from https://www.linkedin.com/pulse/secrets-chatgpts-ai-training-look-high-tech-hardware-behind-kandel/\n\n[10] Kennedy, P. (2023, June 17). Nvidia Notches a Modest Grace Superchip Win at ISC 2023. ServeTheHome. Retrieved from https://www.servethehome.com/nvidia-notches-a-modest-grace-superchip-win-at-isc-2023-arm-hpe/\n\n[11] MLCommons. (2023, March 8). History. MLCommons. Retrieved from https://mlcommons.org/en/history/\n\n[12] Mohan, R. (2023, June 17). AI chip race heats up as AMD introduces rival to Nvidia technology. Tech Xplore. Retrieved from https://techxplore.com/news/2023-06-ai-chip-amd-rival-nvidia.html\n\n[13] Moore, S. (2022). MLPerf Rankings 2022. IEEE Spectrum. https://spectrum.ieee.org/mlperf-rankings-2022\n\n[14] Narendran, S. (2023, May 11). Every major AI feature announced at Google I/O 2023. ZDNet. Retrieved from https://www.zdnet.com/article/every-major-ai-feature-announced-at-google-io-2023/\n\n[15] Nellis, S., & Mehta, C. (2023, June 16). AMD says Meta using 10,000 of its GPUs for AI workload. Retrieved from https://finance.yahoo.com/news/1-amd-says-meta-using-174023713.html\nhttps://www.analyticsvidhya.com/blog/2023/05/meta-reveals-ai-chips-to-revolutionize-computing/\n\n[16] Naik, A. R. (2021, August 4). Explained: NVIDIA's record-setting performance on MLPerf v1.0 training benchmarks. Analytics India Magazine. https://analyticsindiamag.com/explained-nvidias-record-setting-performance-on-mlperf-v1-0-training-benchmarks/\n\n[17] Narasimhan, S. (2022, June 29). NVIDIA partners sweep all categories in MLPerf AI benchmarks. The Official NVIDIA Blog. https://blogs.nvidia.com/blog/2022/06/29/nvidia-partners-ai-mlperf/\n\n[18] Nosta, J. (2023, March 10). Stacked exponential growth: AI is outpacing Moore's law and evolutionary biology. Medium. Retrieved from https://johnnosta.medium.com/stacked-exponential-growth-ai-is-outpacing-moores-law-and-evolutionary-biology-12882c38b68d\n\n[19] Precedence Research. (2022). Artificial Intelligence (AI) in Hardware Market. https://www.precedenceresearch.com/artificial-intelligence-in-hardware-market\n\n[20] Roach, J. (2023, June 17). Meteor Lake VPU: Intel's next-gen chip will have a dedicated AI processor. Digital Trends. Retrieved from https://www.digitaltrends.com/computing/intel-meteor-lake-vpu-computex-2023/\n\n[21] Salvador, D. (2022, April 6). NVIDIA Orin Leaps Ahead in Edge AI, Boosting Leadership in MLPerf Tests. The Official NVIDIA Blog. https://blogs.nvidia.com/blog/2022/04/06/mlperf-edge-ai-inference-orin/\n\n[22] Sharma, S. (2021, December 20). 2021 Was a Breakthrough Year for AI. VentureBeat. Retrieved from https://venturebeat.com/ai/2021-was-a-breakthrough-year-for-ai/\n\n[23] Sweeney, T. [@TimSweeneyEpic]. (2023, April 13). Artificial intelligence is doubling at a rate much faster than Moore’s Law’s 2 years, or evolutionary biology’s 2M years. Why? Because we’re bootstrapping it on the back of both laws. And if it can feed back into its own acceleration, that’s a stacked exponential. Twitter. https://twitter.com/TimSweeneyEpic/status/1646645582583267328\n\n[24] Tardi, C. (2023, June 17). Moore's Law. Investopedia. Retrieved from https://www.investopedia.com/terms/m/mooreslaw.asp\n\n[25] Salvator, D. (2023, April 5). Inference MLPerf AI. The Official NVIDIA Blog. https://blogs.nvidia.com/blog/2023/04/05/inference-mlperf-ai/\n\nPanigrahi, K. K. (2023, January 11). Difference between RISC and CISC. Retrieved from https://www.tutorialspoint.com/difference-between-risc-and-cisc\n\nPlatt, S. (2018, October 16). Metamorphosis of an industry, part two: Moore's Law and Dennard Scaling. Retrieved from https://www.micron.com/about/blog/2018/october/metamorphosis-of-an-industry-part-two-moores-law\n\nBrans, P. Amdahl's law. Retrieved from https://www.techtarget.com/whatis/definition/Amdahls-law\n\nJotrin Electronics. (2022, January 4). A brief history of the development of AI chips. Retrieved from https://www.jotrin.com/technology/details/a-brief-history-of-the-development-of-ai-chips\n\nSong, Y., Dhariwal, P., Chen, M., & Sutskever, I. (2023). Consistency models. arXiv preprint arXiv:2303.01469. Retrieved from https://arxiv.org/abs/2303.01469\n\nBarry, D. J. (2023, April 17). Beyond Moore's Law: New solutions for beating the data growth curve. Microcontroller Tips. https://www.microcontrollertips.com/beyond-moores-law-new-solutions-beating-data-growth-curve/\n\nHennessy, J. L., & Patterson, D. A. (2018). Computer Architecture: A Quantitative Approach (6th ed.). Morgan Kaufmann.\n\nApple Inc. (2021, October 18). Introducing M1 Pro and M1 Max: the most powerful chips Apple has ever built. Apple Newsroom. https://www.apple.com/newsroom/2021/10/introducing-m1-pro-and-m1-max-the-most-powerful-chips-apple-has-ever-built/\n\nApple. (2022, June). Apple unveils M2, taking the breakthrough performance and capabilities of M1 even further. Apple Newsroom. https://www.apple.com/newsroom/2022/06/apple-unveils-m2-with-breakthrough-performance-and-capabilities/\n\nApple Inc. (2023, June). Apple introduces M2 Ultra. Apple Newsroom. https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/\n\nApple Inc. (2022, March). Apple unveils M1 Ultra, the world's most powerful chip for a personal computer. Apple Newsroom. https://www.apple.com/newsroom/2022/03/apple-unveils-m1-ultra-the-worlds-most-powerful-chip-for-a-personal-computer/","metadata":{}},{"cell_type":"markdown","source":"*ChatGPT was used to help me outline the essay in a way that made sense, refine ideas, summarize articles to help me better understand hardware performance improvements, and format the sources into APA format. Google's BARD was used as a validator to make sure my summaries were true and accurate.*","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\")\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T19:28:34.111181Z","iopub.execute_input":"2023-06-17T19:28:34.111669Z","iopub.status.idle":"2023-06-17T19:28:34.127185Z","shell.execute_reply.started":"2023-06-17T19:28:34.111633Z","shell.execute_reply":"2023-06-17T19:28:34.125354Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"             type                                              value\n0  essay_category  'copy/paste the exact category that you are su...\n1       essay_url  'http://www.kaggle.com/your_username/your_note...\n2   feedback1_url  'http://www.kaggle.com/.../your_1st_peer_feedb...\n3   feedback2_url  'http://www.kaggle.com/.../your_2nd_peer_feedb...\n4   feedback3_url  'http://www.kaggle.com/.../your_3rd_peer_feedb...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>essay_category</td>\n      <td>'copy/paste the exact category that you are su...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>essay_url</td>\n      <td>'http://www.kaggle.com/your_username/your_note...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feedback1_url</td>\n      <td>'http://www.kaggle.com/.../your_1st_peer_feedb...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feedback2_url</td>\n      <td>'http://www.kaggle.com/.../your_2nd_peer_feedb...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feedback3_url</td>\n      <td>'http://www.kaggle.com/.../your_3rd_peer_feedb...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val = [\"'Other'\", \"http://www.kaggle.com/your_username/your_public_notebook\",\n      \"http://www.kaggle.com/.../your_1st_peer_feedback\",\n      \"http://www.kaggle.com/.../your_2nd_peer_feedback\",\n      \"http://www.kaggle.com/.../your_3rd_peer_feedback\"]\nsubmission_df.value = val\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T19:28:36.005116Z","iopub.execute_input":"2023-06-17T19:28:36.005553Z","iopub.status.idle":"2023-06-17T19:28:36.013559Z","shell.execute_reply.started":"2023-06-17T19:28:36.005520Z","shell.execute_reply":"2023-06-17T19:28:36.012635Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T19:28:38.507738Z","iopub.execute_input":"2023-06-17T19:28:38.508215Z","iopub.status.idle":"2023-06-17T19:28:38.520785Z","shell.execute_reply.started":"2023-06-17T19:28:38.508181Z","shell.execute_reply":"2023-06-17T19:28:38.519413Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"             type                                              value\n0  essay_category                                            'Other'\n1       essay_url  http://www.kaggle.com/your_username/your_publi...\n2   feedback1_url   http://www.kaggle.com/.../your_1st_peer_feedback\n3   feedback2_url   http://www.kaggle.com/.../your_2nd_peer_feedback\n4   feedback3_url   http://www.kaggle.com/.../your_3rd_peer_feedback","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>essay_category</td>\n      <td>'Other'</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>essay_url</td>\n      <td>http://www.kaggle.com/your_username/your_publi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feedback1_url</td>\n      <td>http://www.kaggle.com/.../your_1st_peer_feedback</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feedback2_url</td>\n      <td>http://www.kaggle.com/.../your_2nd_peer_feedback</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feedback3_url</td>\n      <td>http://www.kaggle.com/.../your_3rd_peer_feedback</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}