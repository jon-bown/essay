{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-17T19:22:44.859402Z","iopub.status.busy":"2023-06-17T19:22:44.857546Z","iopub.status.idle":"2023-06-17T19:22:44.899172Z","shell.execute_reply":"2023-06-17T19:22:44.898102Z","shell.execute_reply.started":"2023-06-17T19:22:44.859300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\n","/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\n","/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\n"]}],"source":["import numpy as np\n","import pandas as pd"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Working Title - \"State of AI: Hardware Advances 2021-2023\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Introduction"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The past two years have seen an unprecedented acceleration in the capability of the hardware behind Artificial Intelligence (AI) systems. Tim Sweeney, CEO of Epic Games described this transition in a recent tweet. He stated, \"Artificial intelligence is doubling at a rate much faster than Moore’s Law’s 2 years, or evolutionary biology’s 2M years. Why? Because we’re bootstrapping it on the back of both laws. And if it can feed back into its own acceleration, that’s a stacked exponential.\" (Sweeney, 2023). \n","\n","In the tweet he quoted, OpenAI announced the release of an implementation of Consistency Models, a new type of generative model that achieves high sample quality without the need for adversarial training (Song, Dhariwal, Chen, & Sutskever, 2023). This innovation is a significant breakthrough because adversarial training, a key component of current generative AI methods, can be very computationally demanding. The ability to produce high-quality samples without the need for adversarial training indicates significant efficiency gains in learning models, accomplished without necessitating hardware innovations. This represents another large step forward in AI capabilities and aligns with Sweeney's point about the rapid rate of AI advancement. With AI's unique ability to contribute to the optimization of its own development, from refining model architectures to enhancing hardware design, it distinctly sets itself apart from traditional computer hardware development. It is this self-augmenting capacity of AI that likely leads to what Sweeney refers to as the \"stacked exponential\" growth.\n","\n","Traditionally, Moore's Law, which predicts a 2-year doubling of transistors integrated onto a circuit, has served as the benchmark for anticipated growth in computing power. However, in the last two years we have seen a surge in acceleration hardware that dramatically outpaces this prediction (Moore, 2022). What advancements in AI related computer hardware have been made in the last two years? What are the factors driving the pace of hardware innovation? How can we measure the acceleration? What does the acceleration mean for the future of AI? While it is impossible to cover every development, this essay will explore these questions in order to encapsulate the state of AI in 2023."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Historical Context"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The term \"artificial intelligence\" was first used by scientists John McCarthy, Claude Shannon, and Marvin Minsky at the Dartmouth Conference in 1956. With optimistic expectations for the field's potential, Minsky boldly proclaimed in a 1970 machines with average human intelligence would exist in the near future. The hype surrounding this forecast ignited an investment wave that lasted over a decade, which culminated in an AI bubble. However, when this bubble burst in the early 1980s, AI development regressed back to research labs, and the field entered what is now referred to as the \"AI Winter\". (Jotrin Electronics, 2022).\n","\n","In the earlier years of computer hardware development, Central Processing Units (CPUs) were the primary source of innovation around computing capabilities. Moore's Law had been the guiding principle for the advancement of these CPUs, predicting a steady pace of growth. However, the demands of AI computations quickly exceeded the capabilities of these CPU architectures which caused much slower progress than originally anticipated by Minsky. \n","\n","The advent of the Graphics Processing Unit (GPU) by Nvidia in 1999 marked a significant change in the computing landscape. Initially used for 3D graphics in video games, their potential in AI model training wasn't recognized until a decade later. The use of GPUs for model training marked an acceleration in AI hardware far surpassing the advancements predicted by Moore's Law.\n","\n","This acceleration reached a tipping point in 2012 with the advent of AlexNet, a groundbreaking neural network model powered by Nvidia's GPU hardware, which won the ImageNet competition by delivering a record-setting image recognition accuracy (Amodei & Hernandez, 2018).\n","\n","The use of hardware in AI has since become a key factor in the speed and efficiency of model training and deployment. In response to the limitations of GPUs, AI-assisted chip design emerged around 2018, leading to innovations like Google's framework for optimizing chip design through a game framework (Goldie & Mirhoseini, 2020) and Synopsis's DSO.ai, the \"worlds first autonomous AI application for chip design\" (\"Synopsys,\" n.d.).\n","\n","The ongoing global chip shortage, which began with the COVID-19 pandemic in 2020, further underscores the need for innovations in semiconductor production. This shortage and the accompanying surge in demand for consumer electronics have elevated the priority of smarter semiconductor manufacturing, setting the stage for significant advancements in AI hardware between 2021-2023 (Appenzeller, Bornstein, & Casado, 2023)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# AI Hardware Advancements (2021-2023)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2021"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we entered the new decade, the stage was set for AI hardware to take a giant leap forward. Not only was the true power about to be put to the test with the first version of Machine Learning Performance benchmark tests (MLPerf v1.0) on the horizon, there was significant capital allocated to AI hardware development. The amount of capital invested in AI hardware companies globally almost doubled from 36 billion USD in 2020 to 68 billion USD in 2021. Market research reports later found that in 2021 the highest demand in AI hardware was for processors (65%) rather than storage or network devices (Precedence Research, 2022). The only issue with that is while processing power is clearly outperforming expectations, network and storage devices are increasingly the bottleneck of even greater performance (Vellante & Floyer, 2021)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","<table>\n","  <tr>\n","    <td align=\"center\">\n","      <img src=\"https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Share-By-Type-2021.jpg\" width=\"400\">\n","    </td>\n","    <td align=\"center\">\n","      <img src=\"https://d2axcg2cspgbkk.cloudfront.net/wp-content/uploads/Breaking-Analysis_-Moores-Law-is-Accelerating-and-AI-is-Ready-to-Explode-3.jpg\" width=\"400\">\n","    </td>\n","  </tr>\n","</table>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["2021 started off with the release of DALL-E by OpenAI in January. DALL-E, a multimodal AI system, distinguishes itself by generating images from text descriptions. Although not a hardware advancement per se, DALL-E's significance in the computational domain is undeniable. It merges two of the most computationally intensive fields in AI: computer vision and natural language processing. To train models like DALL-E and its underlying model, GPT-3, and deep learning models for images, substantial hardware resources are necessary. Moreover, to operate at scale, the model depends on a potent combination of efficient processing power, robust networking capabilities, and high-speed storage hardware. While the exact hardware setup of DALL-E haven't been disclosed, recent attempts to replicate DALL-E on a much smaller scale have shown the complexity of end-to-end hardware setup (Cuenca, 2023).\n","\n","In February, Google released TensorFlow 3D, designed to help businesses develop and train models capable of comprehending 3D scenes. This offering signified an expansion in the AI model development ecosystem, with TensorFlow employing the raw power of GPUs for model training.\n","\n","March saw a landmark collaboration between Nvidia and Harvard, with the development of an AI toolkit called AtacWorks. This toolkit was a testament to Nvidia's determination to tailor AI hardware to handle complex tasks such as genome analysis, thus significantly reducing associated costs and time (Sharma, 2021).\n","\n","The Nvidia Grace CPU was announced in April, the company's first data center CPU, was designed to address the computational requirements of advanced applications such as natural language processing, recommender systems, and AI supercomputing that analyze large datasets. Grace combined energy-efficient Arm CPU cores with a unique low-power memory subsystem to deliver high performance with remarkable efficiency. This Arm-based processor aimed to provide a ten-fold performance increase for systems training large AI models, compared to leading servers at the time. Notably, the Swiss National Supercomputing Centre (CSCS) and the U.S. Department of Energy’s Los Alamos National Laboratory have plans to build Grace-powered supercomputers to support scientific research efforts (Nvidia, 2021).\n","\n","In May, Google announced the introduction of their fourth-generation TPUs, for AI and ML workloads. TPUs, designed specifically to optimize AI computation, stood as Google's response to the rising dominance of GPUs. Another major announcement came from Berkeley Lab’s National Energy Research Scientific Computing Center (NERSC), the Perlmutter supercomputer, built by HPE in collaboration with Nvidia and AMD features around 6,159 Nvidia A100 GPUs and roughly 1,500 AMD Milan CPUs, collectively providing an impressive 3.8 exaflops of theoretical \"AI performance\". It has since been instrumental in mapping the visible universe spanning 11 billion light years by processing data from the Dark Energy Spectroscopic Instrument (DESI), with early benchmarking revealing up to 20X performance speedups using the GPUs, thus reducing computational timeframes from weeks or months to merely hours (HPC Wire, 2021).\n","\n","June brought another pivotal development when Mythic launched an AI processor that required ten times less power than a conventional system-on-chip or GPU. This introduction marked a shift towards creating more energy-efficient hardware solutions for AI, an important consideration as energy costs and environmental impacts become more of a concern (Sharma, 2021). Google published a paper in Nature detailing their approach to using AI for the floor planning stage of chip design (Mirhoseini et al., 2021). This paper was the formalization of their 2020 blog post about AI powered chip design and made the findings more transparent. They also revealed that their fourth generation TPU, released just one month earlier, was designed using this new deep reinforcement learning technique (Vincent, 2021).\n","\n","Following the excitement of AI powered chip design in June, July brought the release of the Cerebrus platform from Cadence. The Cerebrus Intelligent Chip Explorer tool leverages ML to enhance the process of chip design, making engineers remarkably more productive. The introduction of ML has added an additional layer of automation to the design process, resulting in up to 10 times improved productivity per engineer and yielding a 20% enhancement in power, performance, and chip area (Takahashi, 2021).\n","\n","In October, Apple also continued upgrades to the M1 series chips released only a year earlier and already touted as the most powerful chips Apple had ever built. Most notably, both the M1 Pro and M1 Max chips came equipped with the standard 16-core Neural Engine but further enhanced for accelerating on-device ML, indicative of Apple's investment in advancing ML technology through their existing products (\"Introducing M1 Pro and M1 Max,\" 2021). While Apple is not traditionally viewed as a trailblazer in the AI domain, the company has been concentrating on enhancing on-device inference capabilities, a concept known as Edge AI. This approach prioritizes deploying AI applications on devices within the physical world, shifting away from reliance on a centralized cloud server.\n","\n","November showcased advancements from both Nvidia and Amazon. Nvidia announced Omniverse Avatar, a platform harnessing AI hardware capabilities to create real-time interactive avatars, signifying an innovative use of AI hardware. Simultaneously, Amazon unveiled its Graviton3 processors for AI inferencing, illustrating an industry trend towards using AI-specific processors for distinct tasks such as inference (Sharma, 2021).\n","\n","The year 2021 was also an exciting year for AI chip manufacturing startups. In April, Cerebras Systems unveiled an AI supercomputing processor containing an unprecedented 2.6 trillion transistors called WSE-2. This powerful computational device underscores the intensifying demand for advanced AI hardware to keep up with the increasingly intricate tasks (source). In June, Mythic announced the M1076 25 trillion operations per second (TOPS) AI processor which is capable of storing up to 80 million weighted parameters which means that it can run complex AI models without the need for external memory (Mitchell, 2021)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Hardware Summary 2021\n","\n","| Hardware | Company | Key Features |\n","|---|---|---|\n","| DALL-E | OpenAI | End-to-end text-to-image generative model at scale |\n","| TensorFlow 3D | Google | 3D scene comprehension for AI model development |\n","| AtacWorks | Nvidia, Harvard | AI toolkit for genome analysis |\n","| Grace CPU | Nvidia | Company's first data center CPU for AI and high-performance computing |\n","| TPU v4 | Google | Tensor Processing Unit designed for Google's data centers, AI aided design |\n","| Perlmutter Supercomputer | HPE, Nvidia, AMD | Supercomputer with 6,159 Nvidia A100 GPUs and 1,500 AMD Milan CPUs |\n","| Mythic AI Processor | Mythic | AI processor requiring ten times less power than conventional system-on-chip or GPU |\n","| AI Powered Chip Design | Google | AI for floor planning stage of chip design |\n","| Cerebrus Platform | Cadence | ML tool for chip design |\n","| M1 Pro/Max Chip | Apple | 16-core Neural engine optimized for Edge AI/ML acceleration |\n","| Omniverse Avatar | Nvidia | Platform creating real-time interactive avatars |\n","| Graviton3 processors | Amazon | Processors for AI inferencing |\n","| Wafer Scale Engine 2 (WSE-2) | Cerebras Systems | AI supercomputing processor with 2.6 trillion transistors |\n","| M1076 | Mythic | 25 TOPS, up to 80 million weighted parameters |\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2022"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","Throughout 2022, the AI hardware landscape saw an array of impressive launches from leading tech companies and startups alike. Nvidia announced the release of their new DGX Station, DGX-1, and DGX-2 built on state-of-the-art Volta GPU architecture (Gupta, 2022). The system includes the DGX A100, the flagship chip of Nvidia designed for data centers. The chip has integrated eight GPUs and has a GPU memory of 640 GB. Nvidia also announced the release of the H100 data center GPU, the flagship product for the new Hopper architecture. All of these components are specifically designed for deep learning training, accelerated analytics, and inference (Fu, 2022).\n","\n","Just one year after Google made their research and methods for incorporating AI into chip design, Nvidia announced their own incorporation of AI called 'PrefixRL'. Similar methods of reinforcement learning were incorporated into their new Hopper architecture resulting in circuits 25% smaller than those designed by humans with standard EDA tools (Roy, Raiman, & Godil, 2023). Around the same time, an internal struggle emerged at Google questioning the accuracy of findings in their original paper published in 2021 (Dave, 2022). \n","\n","Intel’s Habana Labs released the second generation of their deep learning processors for training and inference — Habana Gaudi2 (Gupta, 2022). IBM launched their first Telum Processor-based system, IBM z16, aimed at improving performance and efficiency for large datasets and featuring on-chip acceleration for AI inference (Fu, 2022).\n","\n","In March and June, Apple also made significant strides in their hardware capabilities, unveiling the M1 Ultra and M2 chip, both next-generation enhancements of their breakthrough M1 chip. The M1 Ultra doubled the number of previous of neural engine cores from 16 to 32 (\"Apple unveils M1 Ultra,\" 2022). The new mac standard neural engine in M2 can process up to 15.8 TOPS, 40% faster than the prior year. (\"Apple unveils M2,\" 2022).\n","\n","In July, IBM and Tokyo Electron made strides in 3D chip stacking by addressing the limitations posed by Moore's law. Silicon carrier wafers, a significant obstacle in 3D chip manufacturing, were at the core of their challenges. The advancements they've introduced are designed to optimize the production process, with the added advantage of potentially alleviating the global chip shortage (Peckham, 2022).\n","\n","By mid-2022 we had a pretty good understanding of the state of the market for the prior year and where things were headed. The AI hardware market was valued at 10 billion USD in 2021 and was projected to grow to almost 90 billion USD by 2030 (Precedence Research, 2022)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","      <img src=\"https://www.precedenceresearch.com/insightimg/Artificial-Intelligence-in-Hardware-Market-Size-2021-to-2030.jpg\" width=\"400\"/>\n","</div>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","On AI Day in September, Tesla revealed its powerful Dojo chip, designed for faster training and inference in self-driving cars (Gupta, 2022). AMD, though not traditionally focused on AI, released Zen 4, a new version of their Zen microarchitecture built on a 5 nm architecture, and introduced a new line of PC processors for ML capabilities (Fu, 2022). Meanwhile, Cerebras Systems launched their AI supercomputer, Andromeda, aiming to accelerate academic and commercial research (Gupta, 2022). In the same vein, SambaNova Systems announced the shipping of the second generation of the DataScale system—SN30. The system, powered by the Cardinal SN30 chip, is built for large models with more than 100 billion (100B) parameters and capable of handling both 2D and 3D images (Fu, 2022).\n","\n","The 2022 AI Hardware Summit in September showcased the emergent trend of Edge AI, pointing out its potential as a major avenue for growth and performance improvement. Edge AI has seen remarkable advancement due to the maturation of deep learning and enhanced computing power. Edge AI has enormous advantages such as reduced latency, better privacy, and reduced energy consumption. Furthermore, a noticeable shift was identified toward TPUs in Edge AI, with more vendors beginning to adopt TPUs as AI accelerators. The Summit also highlighted how AI chips have now advanced to the level of detecting human emotions, emphasizing the impressive strides being made in object detection (Fu, 2022). \n","\n","In October, AWS announced the general availability of Amazon EC2 Trn1 instances, which are powered by AWS Trainium chips, build specifically for training high-performance ML applications in the cloud. Trn1 instances claim up to a 50% cost savings over comparable GPU-based EC2 instances for training large language models (Amazon Web Services, 2022). A month later, at AWS re:Invent 2022, Amazon EC2 Inf2 were made generally available powered by the AWS Inferentia2, an ML accelerator optimized for inference. Inf2 boasts the ability to deploy a 175B parameter model, such as GPT-3, in a single server. These two chip architectures represent a shift from using general purpose hardware to using hardware custom built to the specific phase of the system  (Liu, 2022).\n","\n","The November 2022 release of ChatGPT by OpenAI has highlighted the essential role of hardware in driving AI performance. The high-level functionality of ChatGPT demands significant memory and storage capacity. For instance, this system was trained on an extensive network of 10,000 Nvidia A100 HPC (high-performance computing) accelerators, each of which is a $12,500 tensor core GPU (Kandel, 2023). A case in point is the third version of GPT, GPT-3, which features an astounding 175B parameters and calls for a data capacity of 45 terabytes during its training stage. Training these large models often requires specialized hardware and would not be possible without prior advancements in GPUs or TPUs that can handle a large amount of data and perform parallel computations. Moreover, the inference stage often requires powerful servers for hosting the models, as well as efficient hardware capable of quickly processing requests in real-time.  The success of ChatGPT underscores the intertwined relationship between AI software and hardware advancements, where each drives progress in the other."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Hardware Summary 2022\n","\n","| Hardware | Company | Key Features |\n","|---|---|---|\n","| DGX Station, DGX-1, DGX-2, DGX-A100 | Nvidia | AI supercomputers built on Volta GPU architecture for deep learning, analytics and inference |\n","| H100 data center GPU | Nvidia | Flagship product built on the new Hopper architecture, ideal for large-scale ML and deep learning workloads, designed with PrefixRL. |\n","| Habana Gaudi2 | Intel | Deep learning processor for training and inference, built with 7nm technology |\n","| IBM z16 | IBM | First Telum Processor-based system, for improving performance and efficiency for large datasets, features on-chip acceleration for AI inference |\n","| M1 Ultra, M2 | Apple | 32-core pro model neural engine, 40% faster standard Neural Engine over previous year, 15.8 trillion operations per second |\n","| 3D Breakthroughs | IBM/T.E. | 3D chip enabled silicon carrier wafers |\n","| Dojo Supercomputer | Tesla | Revealed for faster training and inference in self-driving cars, claims to outperform multiple GPUs |\n","| Zen 4 | AMD | Microarchitecture built on a 5 nm architecture, introduced for ML capabilities |\n","| Andromeda Supercomputer | Cerebras Systems | Combines 16 Cerebras CS-2 systems for academic and commercial research, performs one quintillion OPS |\n","| SN30 Datascale | SambaNova Systems | Second generation of the DataScale system, powered by Cardinal SN30 chip, built for large models with more than 100B parameters |\n","| EC2 Trn1, EC2 Inf2 | AWS | Cloud instances powered by latest Trainium, Inferentia chips, built specifically for training & inference |"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2023"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The boom in AI hardware in 2023 is characterized by a proliferation of new platforms engineered for high performance, extreme scalability, energy efficiency, and sophisticated deep learning techniques. These advancements have unlocked new frontiers in the AI and ML landscape, with significant contributions coming from industry powerhouses such as Google, Nvidia, Intel, AMD, Apple, and Meta.\n","\n","The late 2022 release of OpenAI's ChatGPT has sparked a surge in AI advancements over the following six months, primarily fueled by an increased demand for advanced GPU hardware. The competitive landscape, featuring key players like Google with their Bard powered by PaLM 2, Microsoft's Bing AI, and Meta's LLaMA, has been driven by the development of large language models (LLMs). Latest AI hardware's power and efficiency is crucial for training large language models (LLMs), broadening their practical uses. However, hardware is just one facet of product development. Choices about parameter and data size critically shape the design, affecting everything from hardware requirements to training duration and model performance. For instance, the implications of choosing 175B parameters for OpenAI's GPT-3, 170 trillion for GPT-4, versus LLaMA's 65B parameters are considerable.\n","\n","The 2023 developer conferences from Google, Apple, and Microsoft gave us a preview of the types of advancements we are going to see in the latter end of the year and beyond. Google I/O, Microsoft Build, and Apple WWDC each highlighted advancements in AI hardware and software. Google and Microsoft are fully embracing and participating in the AI race with new virtual assistants, product features, and open LLMs just to name a few. Apple, a bit more withdrawn from the hype surrounding AI, did not once mention the term \"artificial intelligence\" at WWDC (Greenburg, 2023). Instead they unveiled numerous software improvements for ML across the device ecosystem along with the upgraded M2 Ultra's 32-core neural engine touted as 40% than the prior year 32-core model. (\"Apple introduces M2 Ultra,\" 2023).\n","\n","Google made a giant leap in its Cloud TPU v4, offering a staggering 10x increase in ML system performance compared to its predecessor, TPU v3. With innovative interconnect technologies and domain-specific accelerators, the TPU v4 not only amplifies performance, but it also champions energy efficiency, leading to a reduction in CO2 emissions. Notably, the TPU v4 is tailored for LLMs such as LaMDA, MUM, and PaLM, with the PaLM model delivering 57.8% of peak hardware floating-point performance over 50 days of training on the TPU v4 (Jouppi & Patterson, 2022).\n","\n","Nvidia marked a substantial milestone with its Grace CPU Superchips, finding a place in the UK-based Isambard 3 supercomputer. This setup, featuring 384 Arm-based Nvidia Grace CPU Superchips, commands a total core count exceeding 55,000. It delivers FP64 performance within a remarkable power envelope of under 270kW. The incorporation of Arm Neoverse V2 cores offers a high-performance edge, as the Grace chips are projected to have superior speed and memory bandwidth compared to their counterparts (Kennedy, 2023).\n","\n","Intel, with its Meteor Lake chips, embedded Vision Processing Units (VPUs) across all variants, thereby offloading AI processing tasks from the CPU and GPU to the VPU. This move resulted in increased power efficiency and ability to handle complex AI models, providing benefits for power-hungry applications such as Adobe suite, Microsoft Teams, and Unreal Engine (Roach, 2023).\n","\n","AMD introduced an AI chip called MI300X, described as \"the world's most advanced accelerator for generative AI\". This introduction is expected to compete head-on with Nvidia's AI chips and generate interest from major cloud providers. Simultaneously, AMD initiated high-volume shipping of a general-purpose central processor chip named \"Bergamo\", adopted by Meta Platforms and others for their computing infrastructure (Mohan, 2023).\n","\n","Meta made its foray into AI hardware by unveiling its first custom-designed chips, the Meta Training and Inference Accelerator (MTIA) and the Meta Scalable Video Processor (MSVP). These chips, optimized for deep learning and video processing, underpin Meta's plans for a next-gen data center optimized for AI, illustrating its dedication to crafting a fully integrated AI ecosystem (Khare, 2023).\n","\n","Although Nvidia currently has 90% AI computing market share, companies like Cerebras, AMD, Intel, IBM, and another startup called Groq are determined to chip away at that lead. Groq is a startup founded by a former Google engineer in 2016 but gained recent attention by claiming that it had created a process to move Meta's LLaMA from Nvidia chips over to its own hardware. The complexity of the current AI hardware makes it a tedious task to adapt model architectures to run quickly on new setups (Lee & Nellis, 2023). \n","\n","While these advancements in 2023 are indeed significant, it's important to note that the AI Hardware Summit for the year is yet to occur, indicating that we don't have the full picture of all the developments in the field for this year. As such, the current state of AI hardware in 2023 should be viewed as a work in progress, awaiting further updates and advancements.\n","\n","In regards to AI chip design, the four major players continue to be Synopsis, Cadence, Google, and Nvidia. Althought there haven't been any significant announcements made this year about a new AI designed chips, there is a shifting sentiment that this movement is going more mainstream due to the increase in customer contracts being reported by Synopsis and Cadence. (Ward-Foxton, 2023)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Hardware Summary 2023\n","\n","| Hardware | Company | Key Features |\n","| -------- | ------- | ------------ |\n","| Google Cloud TPU v4 | Google | Exascale ML performance, 4096 chips, dynamic OCS reconfigurability, hardware support for embeddings, 3D torus interconnect |\n","| Interactive LLMs and Assistants | Google (BARD), Microsoft (Bing AI) | Large-scale models designed for interactive and responsive tasks, leveraging the power and efficiency of the latest AI hardware |\n","| M2 Ultra | Apple | 32-core neural engine, 31.6 TOPS |\n","| Nvidia Grace CPU Superchips in Isambard 3 | Nvidia | 384 Arm-based Nvidia Grace CPU Superchips, >55,000 cores, FP64 performance, <270 kW power consumption, Arm Neoverse V2 cores |\n","| Meteor Lake chips with Vision Processing Units (VPUs) | Intel | Embedded VPUs in all chips for increased power efficiency and the ability to handle complex AI models |\n","| MI300X AI Chip and Bergamo Processor | AMD | Introduced the MI300X, the world's most advanced accelerator for generative AI, and started high-volume shipping of the Bergamo central processor chip |\n","| Meta Training and Inference Accelerator (MTIA) and Meta Scalable Video Processor (MSVP) | Meta | Unveiled custom-designed AI chips optimized for deep learning and video processing and discussed plans for a next-gen data center optimized for AI |\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Measuring AI Hardware Progress"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Traditional CPU performance is gauged by clock speed, often denoted in terahertz (THz), while GPUs and Neural Processing Units (NPUs) measure their comprehensive computational power, referred to as \"bulk compute,\" primarily in variants of floating point operations per second (FLOPS) or TOPS. Focusing solely on CPU power or transistor count fails to capture the true extent of recent progress in AI compute. Take, for example, Apple's NPU, also known as the \"neural engine.\" The performance of these chips has seen year-on-year improvements of over 100%, outpacing the progression in their CPU and GPU counterparts. The chart below overlays the historical processing power of Apple's iPhone chips, highlighting the distinct growth trend of the NPU (Vellante & Floyer, 2021)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","      <img src=\"https://d2axcg2cspgbkk.cloudfront.net/wp-content/uploads/Breaking-Analysis_-Moores-Law-is-Accelerating-and-AI-is-Ready-to-Explode-1.jpg\" width=\"450\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["OpenAI in a 2018 publication asserted that three measurable components of an AI system reflect AI's progress over time: algorithmic innovation, data, and available compute for training. OpenAI concurred that the most accurate reflection of AI advancement hinges on the measure of operations per second, which is even more relevant to model training (Amodei & Hernandez, 2018).\n","\n","For decades, Moore's law successfully correlated with not only the growth trend in transistor count but also the advancement in ML when measured by compute. Yet, with the onset of recent tech innovations, this relationship has shown signs of strain. As we moved into the 2020s, the technology community began questioning the continued relevance of Moore's Law. Influential pieces, like \"We're not prepared for the end of Moore's Law\" (Hoffman, 2020), published in the MIT Technology Review, cast doubts over the longevity of this once-dependable guideline in relation to its prediction of transistor count. On the other hand, some argued that Moore's Law was not dead, but its definition needed broadening. It was clear that we needed more comprehensive metrics to gauge computing advancements and accurately predict future trajectories.\n","\n","The charts below illustrate how the growth in computational demand for different AI models has veered ahead of Moore's Law since 2012. The compute utilized in AI training has swelled by over 300,000 times, with a doubling time of merely 3.4 months. If progress had adhered strictly to Moore's Law, the increase would have been limited to a factor of seven (Amodei & Hernandez, 2018). These charts suggest we have truly entered a new era in the pace of AI compute evolvement more closely aligning with the 'stacked exponential' described by Sweeney."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","<table>\n","  <tr>\n","    <td align=\"center\">\n","      <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1506047%2F86778ff808061547b22637c2437454ef%2Fai-and-compute-all.png?generation=1687738766744537&alt=media\" width=\"400\">\n","    </td>\n","    <td align=\"center\">\n","      <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1506047%2F9574381efab68a160ffbfb6297e69b83%2Fai-and-compute-modern-log.png?generation=1687738839069138&alt=media\" width=\"400\">\n","    </td>\n","  </tr>\n","</table>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Apart from FLOPS or TOPS there are several benchmarks that are commonly used today to evaluate the performance of ML/AI hardware specifically. MLPerf is one of the most popular of the last two years (MLCommons, 2023). Developed by a consortium of tech companies in 2018, MLPerf benchmarks measure the speed of ML software and hardware. MLPerf specifically looks at training and inference performance, scalability, and power performance across the hardware landscape according to the requirements of the specific model or task. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## MLPerf: A Closer Look"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Although MLPerf launched in 2018, it wasn't until the very end of 2020 that it was properly scaled and standardized into the ML Commons consortium. This is why the MLPerf tests of 2021 are referred to as MLPerf v1.0. MLPerf consists of eight benchmark tests: image recognition, medical-imaging segmentation, two versions of object detection, speech recognition, natural-language processing, recommendation, and a form of gameplay called reinforcement learning. MLPerf is often referred to as \"the Olympics of machine learning\" because computers and software from 21 different companies compete on any or all the tests (Moore, 2022). This incentivizes hardware companies like Nvidia to put their best foot forward. The results of the June 2022 MLPerf v2.0 benchmark tests were compared to Moore's law to show the unexpected rate of progress achieved in training times alone. The 2022 MLPerf results showed 9-10x increase in training time performance vs 2018 (Moore, 2022)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","      <img src=\"https://spectrum.ieee.org/media-library/a-chart-shows-six-lines-of-various-colors-sweeping-up-and-to-the-right.jpg?id=30049159&width=1580&quality=80\" width=\"450\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Nvidia's AI hardware in the 2023 MLPerf tests (MLPerf v3.0) has shown a considerable performance increase over its 2022 results, reaffirming Tim Sweeney's statement that AI is \"doubling at a rate much faster than Moore’s Law’s 2 years.\" Nvidia is one of the few companies that has consistently submitted MLPerf results for all eight benchmark tests. Exploring their hardware results shows the evolution of AI compute across the ML landscape.\n","\n","In 2022, Nvidia's AI platform, powered by the A100 Tensor Core GPU, demonstrated significant versatility and efficiency across all eight MLPerf benchmarks. It achieved the fastest time to train on four out of eight tests and was found to be the fastest on a per-chip basis on six out of the eight tests. This performance was attributed to full-stack innovations spanning GPUs, software, and at-scale improvements, delivering 23x more performance in 3.5 years since the first MLPerf submission (Salvator, 2022)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","      <img src=\"https://blogs.nvidia.com/wp-content/uploads/2022/04/MLPerf-inference-April-22-FINAL2-1-1536x663.jpg.webp\" width=\"450\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Fast forward to 2023, the results are even more impressive. The newly introduced Nvidia H100 Tensor Core GPUs, designed in part by AI, running on DGX H100 systems, not only achieved the highest performance in every test of AI inference but also saw a performance gain of up to 54% since their debut in September 2022. Furthermore, the Nvidia L4 Tensor Core GPUs, which debuted in the MLPerf tests, ran over 3x the speed of prior-generation T4 GPUs, demonstrating another significant leap in AI performance for 2023 (Salvator, 2023). This unprecedented progress was in part due to Nvidia's Transformer Engine, a testament to the company's commitment to optimizing software and hardware innovations to push the boundaries of AI performance."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","      <img src=\"https://blogs.nvidia.com/wp-content/uploads/2023/04/H100-GPU-inference-performance-MLPerf-1536x857.jpg\" width=\"450\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Specifically, in the healthcare domain, the H100 GPUs have improved performance by 31% since launch on the 3D-UNet benchmark, used for medical imaging. Additionally, the H100 GPUs powered by the Transformer Engine excelled in the BERT benchmark, a transformer-based large language model, significantly contributing to the rise of generative AI (Salvator, 2023). "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Considerations of this Accelerated Pace"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The rapid development of AI, particularly in the last two years, is a testament to the accelerated evolution of computer technology as it relates to ML compute requirements, well beyond the predictions of Moore's Law. This acceleration, grounded in Sweeney's concept of \"stacked exponential acceleration,\" is bootstrapping on previous advancements to enable new levels of efficiency and capabilities that would have been unimaginable in the past. As AI technologies develop at this unpredictable pace, they bring forth both exceptional opportunities and significant challenges. The progression beyond Moore's predictable pace has some positing that AI could potentially reach, or even surpass, human-level intelligence.\n","\n","This fast-paced advancement towards advanced AI has stirred up a wide range of perspectives within the AI community. On one hand, figures like Geoffrey Hinton, Elon Musk, and former Google CEO Eric Schmidt urge caution, highlighting ethical and existential risks in the long run (Gairola, 2023). On the other hand, optimists such as Andrew Ng see the potential of superior AI to drive unprecedented advancements and solve global challenges (Cherney, 2023). However, the trajectory of AI's future remains as relevant and uncertain as when pioneers like Samuel Butler and Alan Turing first contemplated it.\n","\n","As Samuel Butler posited in his 1863 essay, *\"It is said that the power of a machine to imitate human skill in any real sense of the words depends on the degree of skill possessed by the designer or constructor, and that unless he knew a thing himself he could not possibly teach it to his machine\"* (Butler, 1863). Alan Turing, in his seminal work on machine intelligence, echoed this sentiment in his 1950 paper: *\"'It is said that these machines can only do what we know how to order them to perform', a rather begging of the question. It might also be said that we can only do what machines will allow us to do, for this is equally true\"* (Turing, 1950).\n","\n","Whether or not machines will surpass human intelligence is an age old question. The difference between the hype experienced now vs the 1970's is that we have the compute capability to keep up with the hype, suggesting that we are not in a bubble that would lead to another \"AI Winter\". A recent report from ARK shows that Neural Networks have the most influential potential as a catalyst for a host of other technologies (ARK Investment Management LLC, 2023). "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","      <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1506047%2F59209c8272bb9d69862c7ddecb4a70bc%2Fnn_catalyst.png?generation=1687929381525083&alt=media\" width=\"450\">\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Along with this potential for continued innovation surrounding AI, we also have a dramatically different outlook on AI developments by 2030. AI training costs are currently dropping around 70% per year and projected to continue at that same pace (ARK Investment Management LLC, 2023). "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","    <div>\n","    <div style=\"width: 90%;\">\n","      <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1506047%2F735835fa86dba0aa643ee5b8151a91dd%2Fcosttotrain.png?generation=1687929392429753&alt=media\">\n","      </div>\n","    </div>\n","</div>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["While we continue to see hardware shortages and specialized GPUs remaining costly (Vanian, 2023), a recent estimate shows that the cost of AI hardware and software, when measured by relative comput unit (RCU), will continue to decline at a consistent rate. This combination of continued innovations will eventually enable applications like ChatGPT to run inferences at such a low cost that it can be deployeable to the level of Google search (ARK Investment Management LLC, 2023). "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div align=\"center\">\n","    <div style=\"display: flex; justify-content: center;\">\n","        <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1506047%2Fcd48cc2bff57085850ae9c8e3fccc874%2Fcostperinf.png?generation=1687972455714214&alt=media\" width=\"400\" height=\"300\" style=\"margin-right: 10px;\">\n","        <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1506047%2Fc0765a4abc3f6c8ff40c97c2473e2639%2Faihardwarecost.png?generation=1687929416513761&alt=media\" width=\"400\" height=\"300\" style=\"margin-left: 10px;\">\n","    </div>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Looking Ahead"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The AI hardware landscape has changed dramatically throughout the last two years. We have seen significant advancements from companies like Nvidia, Intel, AMD, AWS, OpenAI, Google, Apple, and Meta that have resulted in a dramatically different outlook on computing than the perception coming into the current decade. Looking to the future, innovation will not be defined soley by how quickly we can double the number of transistors on a chip, but rather how the entire system can be optimized through hardware, software, and algorithms for the full range of system requirements. Measuring that progress with task-specific benchmarks like MLPerf will become increasingly more relevant. No longer is every problem or task simply solved with a more powerful CPU, GPU, nor NPU. Significant computational hardware advances seem to be the new normal every year and the current incentives around AI hardware in particular will continue to drive performance innovations at speeds we have not yet experienced in the history of computing."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Sources"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Amazon Web Services. (2022, October 13). Introducing Amazon EC2 TRN1 Instances for High-Performance, Cost-Effective Deep Learning Training. https://aws.amazon.com/about-aws/whats-new/2022/10/ec2-trn1-instances-high-performance-cost-effective-deep-learning-training/\n","\n","Amodei, D., & Hernandez, D. (2018, May 16). AI and Compute. OpenAI. https://openai.com/research/ai-and-compute/\n","\n","Appenzeller, G., Bornstein, M., & Casado, M. (2023, April 27). Navigating the high cost of AI compute. Andreessen Horowitz. https://a16z.com/2023/04/27/navigating-the-high-cost-of-ai-compute/\n","\n","Apple. (2022, June). Apple unveils M2, taking the breakthrough performance and capabilities of M1 even further. Apple Newsroom. https://www.apple.com/newsroom/2022/06/apple-unveils-m2-with-breakthrough-performance-and-capabilities/\n","\n","Apple Inc. (2021, October 18). Introducing M1 Pro and M1 Max: the most powerful chips Apple has ever built. Apple Newsroom. https://www.apple.com/newsroom/2021/10/introducing-m1-pro-and-m1-max-the-most-powerful-chips-apple-has-ever-built/\n","\n","Apple Inc. (2022, March). Apple unveils M1 Ultra, the world's most powerful chip for a personal computer. Apple Newsroom. https://www.apple.com/newsroom/2022/03/apple-unveils-m1-ultra-the-worlds-most-powerful-chip-for-a-personal-computer/\n","\n","Apple Inc. (2023, June). Apple introduces M2 Ultra. Apple Newsroom. https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/\n","\n","ARK Investment Management LLC. (2023, January 31). Big Ideas 2023. https://research.ark-invest.com/hubfs/1_Download_Files_ARK-Invest/Big_Ideas/ARK%20Invest_013123_Presentation_Big%20Ideas%202023_Final.pdf\n","\n","Barry, D. J. (2023, April 17). Beyond Moore's Law: New solutions for beating the data growth curve. Microcontroller Tips. https://www.microcontrollertips.com/beyond-moores-law-new-solutions-beating-data-growth-curve/\n","\n","Butler, S. (1863). Darwin Among the Machines. In The Notebooks of Samuel Butler.\n","\n","Cherney, M. A. (2023, June 6). Andrew Ng says AI poses no extinction risk. Silicon Valley Business Journal. https://www.bizjournals.com/sanjose/news/2023/06/06/andrew-ng-says-ai-poses-no-extinction-risk.html\n","\n","Cuenca, P. (2023, January 25). The Infrastructure Behind Serving DALL-E Mini. Weights & Biases. https://wandb.ai/dalle-mini/dalle-mini/reports/The-Infrastructure-Behind-Serving-DALL-E-Mini--VmlldzoyMTI4ODAy\n","\n","Dave, P. (2022, May 3). Google faces internal battle over research on AI to speed up chip design. Reuters. https://www.reuters.com/technology/google-faces-internal-battle-over-research-ai-speed-chip-design-2022-05-03/\n","\n","Dilmengani, C. (2023, June 17). AI chip makers: Top 10 companies in 2023. https://research.aimultiple.com/ai-chip-makers/\n","\n","Edwards, B. (2023, May 24). The lightning onset of AI—what suddenly changed? An Ars Frontiers 2023 recap. Ars Technica. https://arstechnica.com/information-technology/2023/05/the-lightning-onset-of-ai-what-suddenly-changed-an-ars-frontiers-2023-recap/\n","\n","Freund, K. (2021, August 9). Using AI to help design chips has become a thing. Forbes. https://www.forbes.com/sites/karlfreund/2021/08/09/using-ai-to-help-design-chips-has-become-a-thing/?sh=29e752cb5d9d\n","\n","Fu, J. (2022, September 29). AI frontiers in 2022. Better Programming. https://betterprogramming.pub/ai-frontiers-in-2022-5bd072fd13c\n","\n","Gairola, A. (2023, May 25). Former Google CEO echoes Musk and Hinton's dire warnings on AI becoming existential risk. Benzinga. https://www.benzinga.com/news/23/05/32566930/former-google-ceo-echoes-musk-and-hintons-dire-warnings-on-ai-becoming-existential-risk\n","\n","Goldie, A., & Mirhoseini, A. (2020, April 3). Chip Design with Deep Reinforcement Learning. Google AI Blog. https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html\n","\n","Greenberg, M. (2023, June 6). The best AI features Apple announced at WWDC 2023. VentureBeat. https://venturebeat.com/ai/the-best-ai-features-apple-announced-at-wwdc-2023/\n","\n","Gupta, A. (2022, March 22). Nvidia’s Grace CPU: The ins and outs of an AI-focused processor. Ars Technica. https://arstechnica.com/gadgets/2022/03/nvidias-grace-cpu-the-ins-and-outs-of-an-ai-focused-processor/\n","\n","Hamblen, M. (2023, February 16). ChatGPT runs 10K Nvidia training GPUs with potential for thousands more. Fierce Electronics. Retrieved from https://www.fierceelectronics.com/sensors/chatgpt-runs-10k-nvidia-training-gpus-potential-thousands-more\n","\n","Hennessy, J. L., & Patterson, D. A. (2018). Computer Architecture: A Quantitative Approach (6th ed.). Morgan Kaufmann.\n","\n","Higginbotham, S. (2022, February 14). Google is using AI to design chips for its AI hardware. Protocol. https://www.protocol.com/google-is-using-ai-to-design-chips\n","\n","Hoffman, K. (2020, February 24). We're not prepared for the end of Moore's law. MIT Technology Review. Retrieved from https://www.technologyreview.com/2020/02/24/905789/were-not-prepared-for-the-end-of-moores-law/\n","\n","HPC Wire. (2021, May 27). NERSC debuts Perlmutter, world's fastest AI supercomputer. https://www.hpcwire.com/2021/05/27/nersc-debuts-perlmutter-worlds-fastest-ai-supercomputer/\n","\n","Hruska, J. (2021, June 8). Intel’s 2021-2022 roadmap: Alder Lake, Meteor Lake, and a big bet on EUV. ExtremeTech. https://www.extremetech.com/computing/323126-intels-2021-2022-roadmap-alder-lake-meteor-lake-and-a-big-bet-on-euv\n","\n","Intelligent Computing Lab, Peking University. (2022). Scalable Architecture for Neural Networks. http://nicsefc.ee.tsinghua.edu.cn/projects/neuralscale/\n","\n","Jotrin Electronics. (2022, January 4). A brief history of the development of AI chips. Retrieved from https://www.jotrin.com/technology/details/a-brief-history-of-the-development-of-ai-chips\n","\n","Jouppi, N., & Patterson, D. (2022, June 29). TPU v4 enables performance, energy, and CO2e efficiency gains. Google Cloud Blog. Retrieved from https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains\n","\n","Kandel, A. (2023, April 7). Secrets of ChatGPT's AI Training: A Look at the High-Tech Hardware Behind It. Retrieved from https://www.linkedin.com/pulse/secrets-chatgpts-ai-training-look-high-tech-hardware-behind-kandel/\n","\n","Kaur, D. (2021, November 3). Here's what the 2021 global chip shortage is all about. Tech Wire Asia. https://techwireasia.com/2021/11/heres-what-the-2021-global-chip-shortage-is-all-about/\n","\n","Kennedy, P. (2023, June 17). Nvidia Notches a Modest Grace Superchip Win at ISC 2023. ServeTheHome. Retrieved from https://www.servethehome.com/nvidia-notches-a-modest-grace-superchip-win-at-isc-2023-arm-hpe/\n","\n","Khare, Y. (2023, June 16). Meta Reveals AI Chips to Revolutionize Computing. Analytics Vidhya. Retrieved from https://finance.yahoo.com/news/1-amd-says-meta-using-174023713.html\n","https://www.analyticsvidhya.com/blog/2023/05/meta-reveals-ai-chips-to-revolutionize-computing/\n","\n","Kharpal, A. (2023, May 28). Europe's bold $145 billion plan to rival U.S., Asian chip giants could 'fail,' experts warn. CNBC. https://www.cnbc.com/2023/05/28/europe-chip-strategy-could-fail-experts-warn.html\n","\n","Lee, J., & Nellis, S. (2023, March 9). Groq adapts Meta's chatbot to its own chips in race against Nvidia. Reuters. https://www.reuters.com/technology/groq-adapts-metas-chatbot-its-own-chips-race-against-nvidia-2023-03-09/\n","\n","Liu, M. (2022). Get the Latest from re:Invent 2022. AWS re:Post. https://repost.aws/articles/ARWg0vtgR7RriapTABCkBnng/get-the-latest-from-re-invent-2022\n","\n","Mack, M. (2019). Fifty Years of Moore's Law. IEEE Transactions on Semiconductor Manufacturing, 24(2), 202-207.\n","\n","Martin, C. (2023, April 12). China’s chip ambitions hit by US sanctions, but Beijing remains determined to catch up. South China Morning Post. https://www.scmp.com/news/china/diplomacy/article/3130528/chinas-chip-ambitions-hit-us-sanctions-beijing-remains\n","\n","McKenzie, J. (2023, June 20). Moore’s law: further progress will push hard on the boundaries of physics and economics. Physics World. https://physicsworld.com/a/moores-law-further-progress-will-push-hard-on-the-boundaries-of-physics-and-economics/\n","\n","Mitchell, R. (2021, June 19). Mythic announces latest AI chip M1076. Electropages. https://www.electropages.com/blog/2021/06/mythic-announces-latest-ai-chip-m1076\n","\n","Mirhoseini, A., Goldie, A., Yazgan, M. et al. (2021). Chip placement with deep reinforcement learning. Nature 595, 230–236. https://www.nature.com/articles/s41586-021-03544-w\n","\n","MLCommons. (2023, March 8). History. MLCommons. Retrieved from https://mlcommons.org/en/history/\n","\n","Mohan, R. (2023, June 17). AI chip race heats up as AMD introduces rival to Nvidia technology. Tech Xplore. Retrieved from https://techxplore.com/news/2023-06-ai-chip-amd-rival-nvidia.html\n","\n","Moore, G. E. (1965). Cramming more components onto integrated circuits. Electronics, 38(8), 114-117.\n","\n","Moore, S. (2022). MLPerf Rankings 2022. IEEE Spectrum. https://spectrum.ieee.org/mlperf-rankings-2022\n","\n","Naik, A. R. (2021, August 4). Explained: Nvidia's record-setting performance on MLPerf v1.0 training benchmarks. Analytics India Magazine. https://analyticsindiamag.com/explained-nvidias-record-setting-performance-on-mlperf-v1-0-training-benchmarks/\n","\n","Narasimhan, S. (2022, June 29). Nvidia partners sweep all categories in MLPerf AI benchmarks. The Official Nvidia Blog. https://blogs.nvidia.com/blog/2022/06/29/nvidia-partners-ai-mlperf/\n","\n","Narendran, S. (2023, May 11). Every major AI feature announced at Google I/O 2023. ZDNet. Retrieved from https://www.zdnet.com/article/every-major-ai-feature-announced-at-google-io-2023/\n","\n","Naval Group. (2023, March 2). AI-powered chip design: A revolution in the semiconductor industry. Naval Group Press Room. https://www.naval-group.com/en/news/ai-powered-chip-design-a-revolution-in-the-semiconductor-industry/\n","\n","Nosta, J. (2023, March 10). Stacked exponential growth: AI is outpacing Moore's law and evolutionary biology. Medium. https://johnnosta.medium.com/stacked-exponential-growth-ai-is-outpacing-moores-law-and-evolutionary-biology-12882c38b68d\n","\n","Nvidia. (2021, April 12). Nvidia Announces CPU for Giant AI and High Performance Computing Workloads. Nvidia Newsroom. https://nvidianews.nvidia.com/news/nvidia-announces-cpu-for-giant-ai-and-high-performance-computing-workloads\n","\n","Nvidia. (2023, May 2). Introducing Nvidia Grace: A CPU specifically designed for giant-scale AI and HPC. Nvidia Newsroom. https://nvidianews.nvidia.com/news/introducing-nvidia-grace-a-cpu-specifically-designed-for-giant-scale-ai-and-hpc\n","\n","Peckham, O. (2022, July 7). IBM, Tokyo Electron Announce 3D Chip Stacking Breakthrough. HPCwire. https://www.hpcwire.com/2022/07/07/ibm-tokyo-electron-announce-3d-chip-stacking-breakthrough/\n","\n","PR Newswire. (2018). Synopsys Unveils Fusion Compiler Enabling 20 Percent Higher Quality-of-Results and 2x Faster Time-to-Results. https://www.prnewswire.com/news-releases/synopsys-unveils-fusion-compiler-enabling-20-percent-higher-quality-of-results-and-2x-faster-time-to-results-300744510.html\n","\n","Precedence Research. (2022). Artificial Intelligence (AI) in Hardware Market. https://www.precedenceresearch.com/artificial-intelligence-in-hardware-market\n","\n","Roach, J. (2023, June 17). Intel thinks your next CPU needs an AI processor — here’s why. Digital Trends. https://www.digitaltrends.com/computing/intel-meteor-lake-vpu-computex-2023/\n","\n","Roy, R., Raiman, J., & Godil, S. (2023, April 5). Designing arithmetic circuits with deep reinforcement learning. Nvidia Developer Blog. Retrieved from https://developer.nvidia.com/blog/designing-arithmetic-circuits-with-deep-reinforcement-learning/\n","\n","Salvator, D. (2022). Nvidia Orin Leaps Ahead in Edge AI, Boosting Leadership in MLPerf Tests. The Official Nvidia Blog. https://blogs.nvidia.com/blog/2022/04/06/mlperf-edge-ai-inference-orin/\n","\n","Salvator, D. (2023a). Inference MLPerf AI. The Official Nvidia Blog. https://blogs.nvidia.com/blog/2023/04/05/inference-mlperf-ai/\n","\n","Sharma, S. (2021, December 20). 2021 Was a Breakthrough Year for AI. VentureBeat. https://venturebeat.com/ai/2021-was-a-breakthrough-year-for-ai/\n","\n","Song, Y., Dhariwal, P., Chen, M., & Sutskever, I. (2023). Consistency models. arXiv preprint arXiv:2303.01469. https://arxiv.org/abs/2303.01469\n","\n","Sparks, E. (2023, January 6). The state of quantum computing in 2023. The Verge. https://www.theverge.com/2023/1/6/22216734/quantum-computing-2023-update\n","\n","Sweeney, T. [@TimSweeneyEpic]. (2023, April 13). Artificial intelligence is doubling at a rate much faster than Moore’s Law’s 2 years, or evolutionary biology’s 2M years. Why? Because we’re bootstrapping it on the back of both laws. And if it can feed back into its own acceleration, that’s a stacked exponential. Twitter. https://twitter.com/TimSweeneyEpic/status/1646645582583267328\n","\n","Synopsys. (n.d.). DSO.ai. https://www.synopsys.com/ai/chip-design/dso-ai.html\n","\n","Takahashi, D. (2021, July 22). AI’s got talent: Meet the new rising star in media and entertainment. VentureBeat. https://venturebeat.com/ais-got-talent-meet-the-new-rising-star-in-media-and-entertainment/\n","\n","Tardi, C. (2023, June 17). Moore's Law. Investopedia. https://www.investopedia.com/terms/m/mooreslaw.asp\n","\n","Turing, A. M. (1950). Computing machinery and intelligence. Mind, 59(236), 433-460. doi:10.1093/mind/LIX.236.433\n","\n","Vanian, J. (2023, March 13). ChatGPT and generative AI are booming, but at a very expensive price. CNBC. Updated 2023, April 17. https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html\n","\n","Varghese, G. (2022, November 28). European Chip Alliance: Uniting for a common cause. EETimes. https://www.eetimes.com/european-chip-alliance-uniting-for-a-common-cause/\n","\n","Vellante, D., & Floyer, D. (2021, April 10). New era of innovation: Moore's law is not dead and AI is ready to explode. SiliconANGLE. https://siliconangle.com/2021/04/10/new-era-innovation-moores-law-not-dead-ai-ready-explode/\n","\n","Vincent, J. (2021, June 10). Google is using machine learning to design its next generation of machine learning chips. The Verge. https://www.theverge.com/2021/6/10/22527476/google-machine-learning-chip-design-tpu-floorplanning\n","\n","Ward-Foxton, S. (2023, February 10). AI-Powered Chip Design Goes Mainstream. EE Times. https://www.eetimes.com/ai-powered-chip-design-goes-mainstream/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["ChatGPT (OpenAI) and Bard (Google) were used as tools while writing this essay. Their contributions were the following:\n","- Narrative: In the early stages of writing many ideas for the structure of the essay were generated, in the latter parts generating ideas for what could be removed.\n","- Research: ChatGPT's Bing integration was used throughout the information gathering process to check for additional sources outside of the ones found using traditional search methods.\n","- Validation: In order to ensure that my summaries of AI hardware advancements were accurate, they were checked against the contents of original sources to ensure summaries reflected the particular advancement or technology.\n","\n","Ultimately the output of these generative tools was always rewritten."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-17T19:28:34.111669Z","iopub.status.busy":"2023-06-17T19:28:34.111181Z","iopub.status.idle":"2023-06-17T19:28:34.127185Z","shell.execute_reply":"2023-06-17T19:28:34.125354Z","shell.execute_reply.started":"2023-06-17T19:28:34.111633Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>essay_category</td>\n","      <td>'copy/paste the exact category that you are su...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>essay_url</td>\n","      <td>'http://www.kaggle.com/your_username/your_note...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>feedback1_url</td>\n","      <td>'http://www.kaggle.com/.../your_1st_peer_feedb...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>feedback2_url</td>\n","      <td>'http://www.kaggle.com/.../your_2nd_peer_feedb...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>feedback3_url</td>\n","      <td>'http://www.kaggle.com/.../your_3rd_peer_feedb...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             type                                              value\n","0  essay_category  'copy/paste the exact category that you are su...\n","1       essay_url  'http://www.kaggle.com/your_username/your_note...\n","2   feedback1_url  'http://www.kaggle.com/.../your_1st_peer_feedb...\n","3   feedback2_url  'http://www.kaggle.com/.../your_2nd_peer_feedb...\n","4   feedback3_url  'http://www.kaggle.com/.../your_3rd_peer_feedb..."]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["submission_df = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\")\n","submission_df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-17T19:28:36.005553Z","iopub.status.busy":"2023-06-17T19:28:36.005116Z","iopub.status.idle":"2023-06-17T19:28:36.013559Z","shell.execute_reply":"2023-06-17T19:28:36.012635Z","shell.execute_reply.started":"2023-06-17T19:28:36.005520Z"},"trusted":true},"outputs":[],"source":["val = [\"'Other'\", \"http://www.kaggle.com/your_username/your_public_notebook\",\n","      \"http://www.kaggle.com/.../your_1st_peer_feedback\",\n","      \"http://www.kaggle.com/.../your_2nd_peer_feedback\",\n","      \"http://www.kaggle.com/.../your_3rd_peer_feedback\"]\n","submission_df.value = val\n","submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-17T19:28:38.508215Z","iopub.status.busy":"2023-06-17T19:28:38.507738Z","iopub.status.idle":"2023-06-17T19:28:38.520785Z","shell.execute_reply":"2023-06-17T19:28:38.519413Z","shell.execute_reply.started":"2023-06-17T19:28:38.508181Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>essay_category</td>\n","      <td>'Other'</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>essay_url</td>\n","      <td>http://www.kaggle.com/your_username/your_publi...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>feedback1_url</td>\n","      <td>http://www.kaggle.com/.../your_1st_peer_feedback</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>feedback2_url</td>\n","      <td>http://www.kaggle.com/.../your_2nd_peer_feedback</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>feedback3_url</td>\n","      <td>http://www.kaggle.com/.../your_3rd_peer_feedback</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             type                                              value\n","0  essay_category                                            'Other'\n","1       essay_url  http://www.kaggle.com/your_username/your_publi...\n","2   feedback1_url   http://www.kaggle.com/.../your_1st_peer_feedback\n","3   feedback2_url   http://www.kaggle.com/.../your_2nd_peer_feedback\n","4   feedback3_url   http://www.kaggle.com/.../your_3rd_peer_feedback"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["submission_df.head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
